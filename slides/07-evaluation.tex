\documentclass[svgnames]{beamer}

\usepackage{pri}

\graphicspath{{./}{figures/}{figures/07-evaluation-figs/}} 

\subtitle{Evaluation of IR and IE Systems}


\begin{document}

\maketitle

\begin{frame} \frametitle{Bibliography}

   \begin{block}{}

        \href{http://www.cs.uic.edu/~liub/WebMiningBook.html}{Bing Liu, Web Data Mining: Exploring Hyperlinks, Contents, and
         Usage Data, 2nd edition}.  Chapter 6.  
    
    \end{block}

   \begin{block}{}

    \href{http://www.mir2ed.org/}{Ricardo Baeza-Yates,
            Berthier Ribeiro-Neto, Modern Information Retrieval, 2nd
            edtion}. Chapter 4.

    \end{block}

   \begin{block}{}

    \href{http://nlp.stanford.edu/IR-book/}{Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch√ºtze, Introduction to Information Retrieval.} Chapter 8.

    \end{block}
    
\end{frame}



\section{Evaluation and Relevance}

\begin{frame} \frametitle{IR System Evaluation}
  
  \begin{block}{Why evaluate?}
      \begin{itemize}
      \item Measure the benefit of using an IR system
      \item Measure how well an IR system fulfills its goal
      \item Compare IR systems
      \end{itemize}

  \end{block}

  \begin{block}{What to evaluate?}
      \begin{itemize}
      \item Collection coverage
      \item Processing time
      \item Output presentation
      \item User effort
      \item \emph{Recall and Precision}
      \end{itemize}
  \end{block}

\end{frame}


% ----------------------------------------------------------------------

\begin{frame}
  \frametitle{Elements of an information retrieval performance evaluation experiment}

\begin{block}{The Cranfield Paradigm}

An IR experiment, as devised by Cyril Cleverdon (1950s), must include:

\begin{enumerate}

\item A reference collection
\item Relevance judgments
\item An evaluation metric

\end{enumerate}

\end{block}


\end{frame}

% ----------------------------------------------------------------------

\begin{frame}  \frametitle{Relevant Documents}
  
  \begin{block}{Recall and Precision} 
    Measure the ability of a system to return \emph{relevant} documents.
  \end{block}
  
  \begin{block}{Relevance}
    \begin{itemize}
    \item Subjective notion
    \item Usually \emph{evaluated by a set of experts}
    \end{itemize}
  \end{block}
\end{frame}

% ----------------------------------------------------------------------

\section{Precision vs. Recall}


\begin{frame}
  \frametitle{Evaluating Prediction}

  \begin{center}
    \includegraphics[width=1.0 \textwidth]{precision-recall}
  \end{center}
\end{frame}




\begin{frame}
  \frametitle{Measuring Precision and Recall}
  
  \begin{definition}
    Let $A$ be the set of documents retrieved for query $Q$. \\ 
    % A is the answer-set
    Let $R$ be the set of documents that are relevant to query $Q$.\\
    \emph{Precision} is the proportion of retrieved documents that are
    relevant, i.e.:
    \begin{displaymath}
      Pr = \frac{|R \cap A|}{|A|}
    \end{displaymath}
    \emph{Recall} is the proportion of relevant documents retrieved, i.e.:
    \begin{displaymath}
      Re = \frac{|R \cap A|}{|R|}
    \end{displaymath}
  \end{definition}
\end{frame}

% ----------------------------------------------------------------------

\begin{frame}
  \frametitle{Precision-Recall Curves}
  
  \begin{block}{}
    \begin{itemize}
    \item Retrieved documents are ordered $\Rightarrow$ we are interested in
      measuring how precision changes as recall increases
    \end{itemize}
  \end{block}

  \begin{example}
    Let $A = \{d_1, d_2, d_3, d_4, d_5, d_6, d_7, d_8, d_9, d_{10}\}$ be an
    ordered set of retrieved documents, for a query $Q$. 

    Let $R = \{d_2, d_5,d_8, d_{15}\}$ be the set of relevant documents for query $Q$.
    \begin{center}
      \begin{tabular}{c|c}
        $Re$ & $Pr$ \\\hline
        $0.25$ & $0.50$ \\
        $0.50$ & $0.40$ \\
        $0.75$ & $0.38$ \\
      \end{tabular}
    \end{center}
  \end{example}
\end{frame}

% ----------------------------------------------------------------------

\begin{frame}
  \frametitle{Interpolated Precision-Recall}
  
  \begin{block}{}
    \begin{itemize}
    \item Precision is usually measured at 10 standard recall points: $0\%,
      10\%, 20\%, ..., 90\%, 100\%$
    \item Precision at $r\%$ recall is defined as
      \begin{displaymath}
        P(r) = \max_{i \geq r}P(i)
      \end{displaymath}
    \item Precision is zero after no more relevant documents are found
    \end{itemize}
  \end{block}
\end{frame}

% ----------------------------------------------------------------------

\begin{frame}
  \frametitle{Interpolated Precision-Recall (cont.)}
  
%  \begin{example}
    Let $A = \{d_1, d_2, d_3, d_4, d_5, d_6, d_7, d_8, d_9, d_{10}\}$ be an
    ordered set of retrieved documents, for a query $Q$. Let $R = \{d_2, d_5,
    d_8, d_{15}\}$ be the set of relevant documents for query $Q$.
    
    \vspace{-4ex}

    \begin{columns}
      \column{0.2\linewidth}

      \begin{center}
          \small
          \begin{tabular}{c|c}
            $Re$ & $Pr$ \\\hline
            $0.25$ & $0.50$ \\
            $0.50$ & $0.40$ \\
            $0.75$ & $0.38$ \\
          \end{tabular}
      \end{center}

      \column{0.3\linewidth}

      \begin{center}
        \begin{tabular}{c|c}
          $Re$ & $Pr$ \\\hline
          $0.00$ & $0.50$ \\
          $0.10$ & $0.50$ \\
          $0.20$ & $0.50$ \\
          $0.30$ & $0.40$ \\
          $0.40$ & $0.40$ \\
          $0.50$ & $0.40$ \\
          $0.60$ & $0.38$ \\
          $0.70$ & $0.38$ \\
          $0.80$ & $0.00$ \\
          $0.90$ & $0.00$ \\
          $1.00$ & $0.00$
        \end{tabular}
      \end{center}

      \column{0.6\linewidth}
 
%      \resizebox{\linewidth}{!}{\input{pr.tex}}
%      \includegraphics[width=\linewidth]{pr.eps}

          \includegraphics[width=.8\linewidth]{pr.png}\\

    \end{columns}
%  \end{example}
\end{frame}

% ----------------------------------------------------------------------

\begin{frame}
  \frametitle{Interpolated Precision-Recall (cont.)}
  
  \begin{example}
    \begin{center}
      \includegraphics[width=0.8\linewidth]{size90.eps}
    \end{center}
  \end{example}
\end{frame}

% ----------------------------------------------------------------------

\section{Other Measures}

\begin{frame}
  \frametitle{P@$N$, R-precision }
  
  \begin{block}{P@$N$ -- Precision at the $N$-th retrieved document}
    Most commonly used 
  \begin{itemize}
    \item $P@5$, 
    \item $P@10$ 
    \item $P@20$
  \end{itemize}
  Usefull for Web retrieval
  \end{block}

  \begin{block}{}
\textbf{R-precision -} Precision at the $R$-th document, where $R$ is
      the number of relevant documents
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{$F$-measure }
  \begin{block}{Harmonic mean of precision and recall:}
      \begin{displaymath}
        F_\beta = \frac{(1 + \beta^2) \times Pr \times Re}{(\beta^2 \times Pr) + Re}
    \end{displaymath}
  \end{block}

  \begin{block}{Ususally we adopt $F_1$:}
      \begin{displaymath}
        F_1 = \frac{2 \times Pr \times Re}{Pr + Re}
    \end{displaymath}
  \end{block}
\end{frame}





\begin{frame}
  \frametitle{AP, MAP}
  
  \begin{block}{}
    \begin{itemize}
    \item \textbf{AP -} Average of the values for the precision at each recall
        point
        \begin{displaymath}
            AP = \frac{\sum_{i=1}^N Pr@i \times R_i}{|R|}
        \end{displaymath}
        where $R_i = 1$ if document at rank $i$ is relevant and $R_i = 0$
        otherwise.
        % https://en.wikipedia.org/wiki/Information_retrieval#Mean_average_precision
    \item \textbf{MAP -} Mean Average Precision
        \begin{displaymath}
            \operatorname{MAP} = \frac{\sum_{q=1}^Q\operatorname{AP}_q}{Q}
        \end{displaymath}
    \item AP can also be interpolated
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Discounted Cumulative Gain}
  
  \begin{block}{Cumulative gain: sum the relevance weights}
    \begin{itemize}
    \item \textbf{DCG -} Discounted cumulative gain
        \begin{displaymath}
            \operatorname{DCG}_p = R_1 + \sum_{i=2}^p\frac{R_i}{\log_2i}
        \end{displaymath}
        where $R_i = 1$ if document at rank $i$ is relevant and $R_i = 0$
        otherwise.
    \item \textbf{nDCG -} Normalized discounted cumulative gain
        \begin{displaymath}
            \operatorname{nDCG}_p = \frac{\operatorname{DCG_p}}{\text{Ideal}\operatorname{DCG}_p}
        \end{displaymath}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{MRR}
  
  \begin{block}{MRR - Mean Reciprocal Rank}
        \begin{displaymath}
            \operatorname{MRR} = \frac{1}{N}\sum_{i=1}^N\frac{1}{\operatorname{rank}_i}
        \end{displaymath}
        where $\operatorname{rank}_1$ is the rank of the first relevant
        document.
  \end{block}
\end{frame}

\begin{frame}
    \frametitle{User Models}
    \begin{itemize}
    \item \emph{Position models}: assume independence among documents in
        different positions and model the examination probability as a function
        of the position
    \item \emph{Cascade Models}: consider the dependency among URLs on a search
        results page --- at each position, the user has a certain probability
        of being satisfied depending on the relevance of the previous documents
    \end{itemize}
    Previous measures assumed a position model; following we show \textit{ERR}, which
    assumes a cascade model.
\end{frame}

\begin{frame} \frametitle{ERR}
\begin{block}{ERR - Expected Reciprocal Rank}
        \begin{eqnarray*}
            \operatorname{ERR} & = & \sum_{i=1}^N \frac{1}{i} {\operatorname{P}}(\text{user stops at position~} i) \\
                               & = & \sum_{i=1}^N \frac{1}{i} \Pi_{j=1}^{i-1}(1 - R_j) R_i
        \end{eqnarray*}
        where $R_i = 1$ if document at rank $i$ is relevant and $R_i = 0$
        otherwise.

        $R_i$ can also be the result of mapping from relevance grades to
        probability of relevance $R_i := \mathcal{R}(g_i)$, where:
        \begin{displaymath}
        \mathcal{R}(g) = \frac{2^g - 1}{2^{g_{\max}}}, \; g \in \{0, \cdots, g_{\max}\}
        \end{displaymath}
  \end{block}
\end{frame}

% http://olivier.chapelle.cc/pub/err.pdf

\section{Ranking Comparison}

\begin{frame}
    \frametitle{Spearman Coefficient}

    \begin{block}{}
        Computes the difference between the positions of a same document in two
        rankings
        \begin{displaymath}
            \rho(X,Y) = 1 - \frac{6\sum_{i=1}^N d_i^2}{N(N^2-1)}
        \end{displaymath}
        where $d_i = \operatorname{rank(X)}_i - \operatorname{rank(Y)}_i$ is
        the difference in rankings of document $i$.
        % https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient
    \end{block}

\end{frame}

\begin{frame}
    \frametitle{Kendall's Tau}

    \begin{block}{}
        Let $(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)$, where each $x_i$ is
        the rank of document $i$ in ranking $X$, and $y_i$ is the rank of
        document $i$ in ranking $Y$.
        \begin{displaymath}
            \tau = \frac{|\text{concordant pairs}| - |\text{discordant pairs}|}{N(N-1)/2}
        \end{displaymath}
        \small
        where a pair $(x_i, y_i)$ is concordant with $(x_j, y_j)$ if either:
        \begin{displaymath}
            \left\{
                  \begin{array}{l}
                    x_i > x_j \wedge y_i > y_j\\
                    x_i < x_j \wedge y_i < y_j
                  \end{array}
            \right.
        \end{displaymath}
        and discordant if either:
        \begin{displaymath}
            \left\{
                  \begin{array}{l}
                    x_i > x_j \wedge y_i < y_j\\
                    x_i < x_j \wedge y_i > y_j
                  \end{array}
            \right.
        \end{displaymath}
        % https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient
    \end{block}

\end{frame}

% ------------------------------------------------------------

\section{Obtaining the Ground Truth}

\begin{frame}
  \frametitle{Reference Collections}

  \begin{block}{}
    \begin{description}
    \item[\href{http://trec.nist.gov/data.html}{TREC}] Various collections of
        documents (Ad hoc, Web, Blog, Clinical Decision Support, ...)
    \item[CACM] Articles from Communications of the ACM
    \item[ISI] Information science papers
    \item[CFC] Cystic Fibrosis Collection
    \item[...]
    \end{description}
  \end{block}

  \begin{block}{}
    \begin{itemize}
    \item Standards for research in IR
    \item Provide sets queries + evaluated documents
    \end{itemize}
  \end{block}
\end{frame}

% ------------------------------------------------------------

\begin{frame}
  \frametitle{Human Experimentation in the Lab}

\begin{itemize}
\item User preferences are affected by the characteristics of the user interface (UI)
\begin{itemize} 
\item For instance, the users of search engines look
  first at the upper left corner of the results page. 
\item Changing the layout is likely to affect the assessment
  made by the users and their behavior.
\end{itemize}
\item Proper evaluation of the user interface requires going beyond
  the framework of the Cranfield experiments
\end{itemize}


\end{frame}


\begin{frame}
  \frametitle{A/B Testing}

\begin{itemize}
\item A/B testing consists of displaying to selected users a modification in the layout of a page
\begin{itemize}
  \item The group of selected users constitute a fraction of all users such as, for instance, 1\%
  \item The method works well for sites with large audiences
\end{itemize}
\item By analysing how the users react to the change, it is possible to analyse if the modification proposed is positive or not
\end{itemize}

\begin{block}{} 
A/B testing provides a form of human experimentation, even if the setting is not that of a lab
\end{block}

\end{frame}


\begin{frame}
  \frametitle{Crowdsoursing}

\centering
\textbf{Amazon Mechanical Turk}
\includegraphics[width=0.8 \textwidth]{Amazon-Mechanical-Turk}\\
{\small \url{https://www.mturk.com}}

\begin{itemize}
    \small
\item The participants execute human intelligence tasks, called HITs, in exchange for small sums of money
\item The tasks are filed by requesters who have an evaluation need
\item While the identity of participants is not known to requesters,
  the service produces evaluation results of high quality (except for \emph{free-loaders, etc})
\end{itemize}

\end{frame}




\begin{frame}
  \frametitle{Evaluation using Clickthrough Data}

\begin{block}{A promising alternative...}
The data can be obtained by observing how frequently the users click on a given document, when it is shown in the answer set for a given query
\end{block}

%%This is particularly attractive because 
\begin{block}{Attractive, because...}
The data can be collected at a low cost without overhead for the users
\end{block}
\end{frame}

\begin{frame} \frametitle{Evaluation using Clickthrough Data (2)}
\begin{block}{Click models}
An accurate user model, which closely reflects users‚Äô interactions with the retrieval system, is essential for developing a good relevance metric from clickthrough data.

~\\
\emph{Example:} Cascade model used in ERR metric, corresponding to
\begin{displaymath}
\Pi_{j=1}^{i-1}(1 - R_j) R_i
\end{displaymath}
where the values $R_i$ (i.e., document satisfies the user with probability $R_i$) can be estimated by maximum likelihood on the click logs.
\end{block}
\end{frame}


\section{Evaluation of Classifiers}

\begin{frame} \frametitle{Classifier Evaluation}
  
  \begin{itemize}
  \item Previous lectures have shown that tasks such as document classification or information extraction from text can be modeled as classification problems
  \begin{itemize}
  \item I.e., techniques in this section also apply to IE systems
  \end{itemize}  

  \item Goal in supervised classification is the minimization of classification error on test data
  
  \item We can evaluate through measures like recall, precision, and accuracy (i.e., one minus error)
  \begin{itemize}
  \item But classification tasks can involve more than two classes (i.e., more than distinguishing relevant from non-relevant)
  \end{itemize}  
  \end{itemize}

 \end{frame}

% % ------------------------------------------------------------

\begin{frame} \frametitle{Confusion Matrix}
  
 \begin{itemize}
 \item $M[i,j]$ is the number of test documents belonging to class $i$ which
     were assigned to class $j$
 \item Perfect classifier: diagonal elements $M[i,i]$ would be nonzero
 \item Example:
     \begin{displaymath}
         \footnotesize
         M = \left\{
             \begin{array}{c|c|c}
               5 & 0 & 0 \\\hline
               1 & 3 & 0 \\\hline
               1 & 2 & 4
             \end{array}
         \right\}
     \end{displaymath}
 \item If $M$ is large, we use
     \begin{displaymath}
         \text{accuracy} = \sum_i M[i,i] / \sum_{i,j} M[i,j]
     \end{displaymath}
 \item Notice that accuracy is not a good measure for {\it small} classes
 \end{itemize}

\end{frame}

% ------------------------------------------------------------

\begin{frame} \frametitle{Micro-Averaged Precision}
  
 In a problem with $n$ classes, let $C_i$ be the number of documents in class
 $i$ and let $C'_i$ be the number of documents estimated to be of class $i$ by
 the classifier
 \begin{itemize}
 \item \emph{Micro-averaged precision} is defined as
   \begin{displaymath}
     \frac{\sum_{i=1}^n C'_i \cap C_i}{\sum_{i=1}^n C'_i}
   \end{displaymath}
 \item \emph{Micro-averaged recall} is defined as
   \begin{displaymath}
     \frac{\sum_{i=1}^n C'_i \cap C_i}{\sum_{i=1}^n C_i}
   \end{displaymath}
 \end{itemize}

 \begin{itemize}
 \item Micro-averaged precision/recall measures correctly classified
   documents, thus favoring large classes
 \end{itemize}
\end{frame}

% ------------------------------------------------------------

\begin{frame} \frametitle{Macro-Averaged Precision}
  
 In a problem with $n$ classes, let $P_i$ and $R_i$ be the precision and
 recall, respectively, achieved by a classifier for class $i$
 \begin{itemize}
 \item \emph{Macro-averaged precision} is defined as
   \begin{displaymath}
     \frac{1}{n}\sum_{i=1}^n P_n
   \end{displaymath}
 \item \emph{Macro-averaged recall} is defined as
   \begin{displaymath}
     \frac{1}{n}\sum_{i=1}^n R_n
   \end{displaymath}
 \end{itemize}

 \begin{itemize}
 \item Macro-averaged precision/recall measures performance per class, giving
   all classes equal importance
\item The \emph{$F_1$ measure} is also commonly used
 \end{itemize}
\end{frame}

% ------------------------------------------------------------

\begin{frame} \frametitle{Multi-Label Scenario }
  
  \begin{itemize}
  \item Quality can be measured by per-instance \emph{recall} and \emph{precision}
    \begin{itemize}
    \item Let $C_d$ be the correct classes for document $d$ and $C'_d$ be the
      set of classes estimated by the classifier
      \begin{displaymath}
        precision = \frac{C'_d \cap C_d}{C'_d}
      \end{displaymath}
      \begin{displaymath}
        recall = \frac{C'_d \cap C_d}{C_d}
      \end{displaymath}
    \end{itemize}
  \end{itemize}
\end{frame}

% ------------------------------------------------------------

\begin{frame}
    \frametitle{Train-Test Split}
    \begin{itemize}
    \item When evaluatig a classifier, you cannot rely on the data used for
        training
        \begin{itemize}
        \item You estimate is likely to be overly optimistic
        \item Your model will tend to \emph{overfit}
        \end{itemize}
    \item Data must be split into a test and training sets
        \begin{itemize}
        \item Common train/test splits: 80\%/20\% or 70\%/30\%
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Cross-Validation}
    \begin{itemize}
    \item Splitting the dataset and testing once may lead to a biased
        evaluation
    \item One way to avoid this is to use \emph{cross-validation}
        \begin{itemize}
        \item Leave-p-out
        \item Leave-one-out
        \item \emph{k-fold}
        \item ...
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{K-Fold Cross-Validation}
    \begin{enumerate}
    \item Split the data into $k$ partitions
    \item For each fold $i \in [1,k]$
        \begin{enumerate}
        \item Train your model using all partitions $P_j,\; j \neq i$
        \item Evaluate your model in partition $P_i$
        \end{enumerate}
    \item Average your evaluation metrics overall all folds
    \end{enumerate}
    \begin{center}
        \includegraphics[width=0.8 \textwidth]{K-fold_cross_validation_EN}
    \end{center}
    \tiny\raggedleft(source of image: wikipedia)
\end{frame}

% https://en.wikipedia.org/wiki/Cross-validation_(statistics)

\section{Evaluation of Clustering}

\begin{frame}
    \frametitle{What to evaluate}
    \begin{itemize}
    \item Goal of clustering: attain \emph{high intra-cluster similarity}
        (documents within a cluster are similar) and \emph{low inter-cluster
          similarity} (documents from different clusters are dissimilar
    \item These are \emph{internal criteria}
    \item Alternatively, we can evaluate the results of the application
        of interest
        \begin{itemize}
        \item E.g. for search result clustering, evaluate search results
        \end{itemize}
    \item Or we can use \emph{external criteria}, comparing the clusters to a
        gold standard
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{External Criteria}
    \begin{itemize}
    \item Clusters are evaluated using a gold standard (as in classification
        problems)
        \begin{itemize}
        \item Each document will be assigned a class
    \end{itemize}
    \item Unlike classification problems, we don't know to which class each
        cluster corresponds
        \begin{itemize}
        \item Thus we cannot directly computed false positives, false
            negatives, etc.
        \end{itemize}
    \item There are many proposals to address this issue
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Purity}
    \begin{block}{}
        For a set $\Omega = \{w_1,w_2,\cdots,w_K\}$ of clusters and a set
        $\mathcal{C} = \{c_1,c_2,\cdots,c_K\}$ of classes, \emph{purity} is
        defined as:
        \begin{displaymath}
            \operatorname{purity}(\Omega, \mathcal{C}) = \frac{1}{N} \sum_k\max_j|w_k \cap c_j|
        \end{displaymath}
        where $N$ is the number of documents, $w_k$ is the set of documents in
        cluster $w_k$, and $c_k$ is the set of documents in class $c_k$.
    \end{block}
    Each cluster is assigned to the class most frequent in the cluster and
    accuracy is measured.
\end{frame}

% https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html

\begin{frame}
    \frametitle{Normalized Mutual Information}
    \begin{itemize}
    \item Purity does not show the tradeoff between quality and the number f
        clusters
        \begin{itemize}
        \item E.g., if each document gets a cluster, purity = 1
        \end{itemize}
    \item \emph{NMI} takes this tradeoff into account
        \begin{itemize}
        \item Measures the increase in the amount of information when we are
            know what the clusters are,
        \item But normalizes it by the entropy of the clusters
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Normalized Mutual Information (cont.)}

    \begin{block}{}
        \begin{displaymath}
        \operatorname{NMI}(\Omega,\mathcal{C}) =
            \frac{I(\Omega,\mathcal{C})}{\big(H(\Omega)+H(\mathcal{C})\big)/2}
        \end{displaymath}
    \end{block}
    \footnotesize
    where
    \begin{block}{}
        \begin{multline*}
            I(\Omega,\mathcal{C}) = \sum_k\sum_jP(w_k \cap c_j)\log\frac{P(w_k \cap c_j)}{P(w_k)P(c_j)} \\
            = \sum_k\sum_j \frac{|w_k \cap c_j|}{N} \log\frac{N |w_k \cap c_j|}{|w_k||c_j|}
        \end{multline*}
    \end{block}
    is the mutual information between clusters and classes and
    \begin{block}{}
        \begin{displaymath}
            H(\Omega) = -\sum_k P(w_k)\log{P(w_k)} = -\sum_k\frac{|w_k|}{N}\log\frac{|w_k|}{N}
        \end{displaymath}
    \end{block}
    is the entropy (equally for $H(\mathcal{C})$)
\end{frame}

\begin{frame}
    \frametitle{Rand Index}
    \begin{itemize}
    \item The \emph{Rand index} views clustering as a series of decisions for
        each of the $N(N-1)/2$ pairs of documents
    \item Decisions can be:
        \begin{itemize}
        \item \textbf{True Positive}: the documents are similar and in the same cluster
        \item \textbf{True Negative}: the documents are \textit{not} similar and in different clusters
        \item \textbf{False Positive}: the documents are \textit{not} similar but in the same cluster
        \item \textbf{False Negative}: the documents are similar but in different clusters
        \end{itemize}
    \item We can considere documentas as \emph{similar if they are in the same class}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Rand Index (cont.)}
    The Rand index simply measures the percentage of correct decisions
    (i.e. the accuracy):
    \begin{block}{}
        \begin{displaymath}
            RI = \frac{TP+TN}{TP+FP+TN+FN}
        \end{displaymath}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{The $F_\beta$ Measure (Again)}
    \begin{itemize}
    \item The Rand index gives equal weight to false positives and false negatives
    \item Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster. 
    \item We can use the F measure to penalize false negatives more strongly
        than false positives, giving more weight to recall
        \begin{itemize}
        \item selecting a value $\beta > 1$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{The $F_\beta$ Measure (cont.)}
    \begin{block}{}
        \begin{displaymath}
            P = \frac{TP}{TP+FP} \qquad R = \frac{TP}{TP+FN}
        \end{displaymath}
    \end{block}
    \begin{block}{}
        \begin{displaymath}
            F_\beta = \frac{(1 + \beta^2)\cdot P \cdot R}{(\beta^2 \cdot P) + R}            
        \end{displaymath}
    \end{block}
    or, using the decision framework:
    \begin{block}{}
        \begin{displaymath}
            F_\beta = \frac{(1 + \beta^2) \cdot TP}{(1 + \beta^2) \cdot TP + \beta^2 \cdot FN + FP}
        \end{displaymath}
    \end{block}
\end{frame}

\finalframe{Questions?}

\end{document}
