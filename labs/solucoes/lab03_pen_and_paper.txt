*** LAB 03 - SOLUTION TO PEN AND PAPER EXERCISE ***

4.1) Part 1: Binary Model - Estimate the parameters of a binary naive Bayes model required for classifying the document

Binary model: Each parameter Theta_c,t indicates the probability that a document d in class c will mention t at least once.

P(d|c,Theta) = Pi_(t in d) Theta_c,t * Pi_(t not in d) (1 - Theta_c,t)

Theta_c,t = N_c,t/N_c = n. of docs in class c containing term t/ n of docs in class c

+-----------+-------------------+-------------------+------------------+
|           | Positive(N_c = 3) | Negative(N_c = 2) | Neutral(N_c = 1) |
+-----------+-------------------+-------------------+------------------+
|                                t in d                                |
+----------------------------------------------------------------------+
|   movie   |        3/3        |        1/2        |         1        |
|   great   |        2/3        |        1/2        |         0        |
|  overall  |        1/3        |        1/2        |         0        |
+-----------+-------------------+-------------------+------------------+
|                              t not in d                              |
+----------------------------------------------------------------------+
|    the    |        2/3        |        1/2        |         0        |
|     is    |        2/3        |        1/2        |         1        |
|  nothing  |        2/3        |        2/2        |         1        |
|    but    |        2/3        |        2/2        |         1        |
|   mixed   |        3/3        |        2/2        |         0        |
|  feelings |        3/3        |        2/2        |         0        |
|   about   |        3/3        |        2/2        |         0        |
|    not    |        3/3        |        1/2        |         1        |
|     so    |        3/3        |        1/2        |         1        |
| fantastic |        2/3        |        2/2        |         1        |
|    good   |        2/3        |        2/2        |         1        |
|  terrible |        3/3        |        1/2        |         1        |
+-----------+-------------------+-------------------+------------------+

P(Positive) = 1/2
P(Negative) = 1/3
P(Neutral) = 0

P(d|Positive, Theta) = 1/3*(2/3)^7*(3/3)^7
P(d|Negative, Theta) = (1/2)^8*(2/2)^7
P(d|Neutral, Theta) = 0

P(c|d) ~ P(d|c)*P(c)
P(Positive|d) = 0.0098
P(Negative|d) = 0.0013
P(Neutral|d) = 0



4.1) Part 2: Estimate the parameters of a Perceptron classifier based on the first 3 training instances, discriminating the positive instances from all other instances

Vocabulary: [the, movie, is, nothing, but, great, mixed, feelings, about, not so]

d_1 = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] Class 1 = A
d_2 = [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0] Class -1 = B
d_3 = [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1] Class -1 = B

t = 0, alpha_0 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

Step 1: d_1 in Pos(A), alpha_0*d_1 = 0 -> alpha_1 = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
Step 2: d_2 in ~Pos(B), alpha_1*d_2 = 2 >= 0 -> alpha_2 = [0, 0, 1, 1, 1, 1, -1, -1, -1, 0, 0]
Step 3: d_3 in ~Pos(B), alpha_2*d_3 = 1 >= 0 -> alpha_3 = [0, 0, 1, 1, 1, 0, -1, -1, -1, -1, -1]


4.2) State which would be the result of applying one iteration of the k-means algorithm over the documents.

Binary document representations

Vocabulary: [shipment, of, gold, damaged, in, fire, delivery, silver, arrived, truck]

d_1 = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
d_2 = [0, 1, 0, 0, 1, 0, 1, 1, 1, 1]
d_3 = [1, 1, 0, 0, 1, 0, 0, 1, 1, 1]
d_4 = [0, 0, 0, 1, 1, 1, 0, 0, 0, 1]

k=2
c_1 = d_2, c_2 = d_3

Round 1:
dist(d_1, Cluster_1) = 8
dist(d_1, Cluster_2) = 6
dist(d_4, Cluster_1) = 6
dist(d_4, Cluster_1) = 6

Assigning clusters:

d_1 -> Cluster_2
d_2 -> Cluster_1
d_3 -> Cluster_2
d_4 -> Cluster_1

Updating clusters' centroids:

c_1 = [0, 0.5, 0, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 1]
c_2 = [1, 1, 0.5, 0.5, 1, 0.5, 0, 0.5, 0.5, 0.5]
