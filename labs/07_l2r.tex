\documentclass[12pt]{article}

\usepackage{prilab}
\usepackage{url}


\begin{document}

\maketitle{Lab 07: Learning to Rank}

The Whoosh search engine provides three different ranking functions: \emph{BM25}, \emph{TF\_IDF} (i.e. cosine similarity) and \emph{Frequency}\footnote{\url{https://whoosh.readthedocs.io/en/latest/api/scoring.html}}. 

The following example code shows how to perform a query using the
TF\_IDF scoring function and obtain the corresponding textual similarity score:
\begin{verbatim}
from whoosh.index import open_dir
from whoosh.qparser import *
ix = open_dir("indexdir")
with ix.searcher(weighting=scoring.TF_IDF()) as searcher:
    query = QueryParser("content", ix.schema, group=OrGroup).parse(u"a query")
    results = searcher.search(query, limit=100)
    for i,r in enumerate(results):
        print r, results.score(i)
\end{verbatim}
A similar procedure can be applied for the remaining ranking functions.

The goal of this exercise is to create a method for scoring the documents that combines the results from these three functions.

\section{}

Using the Whoosh search engine with the document collection of the previous labs (files~\texttt{pri\_cfc.txt} and \texttt{pri\_queries.txt}), implement a script that performs searches and returns the results ordered by a \emph{linear combination} of the three textual similarities presented above.

The rank combination formula should be:
\begin{displaymath}
    \operatorname{score}(q,d) = \alpha_1\operatorname{bm25}(q,d) + \alpha_2\operatorname{cos}(q,d) + \alpha_3\operatorname{freq}(q,d)
\end{displaymath}
where $d$ is the document, $q$ is the query, $\operatorname{bm25}$ is the score obtained using the BM25 ranking function, $\operatorname{cos}$ is the score obtained using the TF\_IDF ranking function, and $\operatorname{freq}$ is the score obtained using the Frequency ranking function.

Experiment with different values for weights $\alpha_1$, $\alpha_2$, and $\alpha_3$ and try to find an improvement in average $F_1$ over the results achieved with each individual ranking function used in isolation.


\section{}

The goal now is to try a more sophisticated approach for combining the ranking functions used in the previous exercise. To this effect we will use a \emph{pointwise Learning to Rank} (L2R) approach.

Our approach consists in training a Logistic Regression classifier\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}} on the set of queries available in \texttt{pri\_queries.txt}. More specifically, you should:
\begin{enumerate}
\item Create a dataset for training and testing your L2R approach:
    \begin{itemize}
    \item Use 50\% of the queries for training and 50\% for testing (you can vary these percentages if you wish);
    \item With the training queries, build the \emph{training dataset}. This dataset should contain, for each \textit{(query $q$, document $d$)} pair, a set of classification instances with the format:
        \begin{displaymath}
            \operatorname{bm25}(q,d), \operatorname{cos}(q,d), \operatorname{freq}(q,d), r
        \end{displaymath}
        where $r = 1$ if document $d$ is relevant for query $q$ and $r=0$ otherwise. You can store this data on a \textit{numpy} array;
    \item Use the same number of relevant and non-relevant documents for each query.
    \end{itemize}
\item Use the training dataset to learn a Logistic Regression classifier:
    \begin{itemize}
    \item The three ranking scores will be your classification features and $r$ will be the target class.
    \end{itemize}
\item Execute the queries on the testing set, using the Logistic Regression classifier as your ranking function, and measure Precision, Recall, and $F_1$:
    \begin{itemize}
    \item To do this, first perform regular searches, using a each ranking function in isolation;
    \item The score of each ranking function will be the classification features and the classifier will return 1 if the document is relevant or 0 if otherwise;
    \item To order the resulting documents, you should use the \emph{probability of the document being relevant}. This can be obtained through the \texttt{predict\_proba} method of the \texttt{LogisticRegression} class.
    \end{itemize}
\end{enumerate}



\section{Pen and Paper Exercise}

TBD

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

