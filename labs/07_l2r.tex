\documentclass[12pt]{article}

\usepackage{prilab}
\usepackage{url}


\begin{document}

\maketitle{Lab 07: Learning to Rank}

The Whoosh search engine provides three different ranking functions: \emph{BM25}, \emph{TF\_IDF} (i.e. cosine similarity) and \emph{Frequency}\footnote{\url{https://whoosh.readthedocs.io/en/latest/api/scoring.html}}. 

The following example code shows how to perform a query using the
TF\_IDF scoring function and obtain the corresponding textual similarity score:
\begin{verbatim}
from whoosh.index import open_dir
from whoosh.qparser import *
ix = open_dir("indexdir")
with ix.searcher(weighting=scoring.TF_IDF()) as searcher:
    query = QueryParser("content", ix.schema, group=OrGroup).parse(u"a query")
    results = searcher.search(query, limit=100)
    for i,r in enumerate(results):
        print r, results.score(i)
\end{verbatim}
A similar procedure can be applied for the remaining ranking functions.

The goal of this exercise is to create a method for scoring the documents that combines the results from these three functions.

\section{}

Using the Whoosh search engine with the document collection of the previous labs (files~\texttt{pri\_cfc.txt} and \texttt{pri\_queries.txt}), implement a script that performs searches and returns the results ordered by a \emph{linear combination} of the three textual similarities presented above.

The rank combination formula should be:
\begin{displaymath}
    \operatorname{score}(q,d) = \alpha_1\operatorname{bm25}(q,d) + \alpha_2\operatorname{cos}(q,d) + \alpha_3\operatorname{freq}(q,d)
\end{displaymath}
where $d$ is the document, $q$ is the query, $\operatorname{bm25}$ is the score obtained using the BM25 ranking function, $\operatorname{cos}$ is the score obtained using the TF\_IDF ranking function, and $\operatorname{freq}$ is the score obtained using the Frequency ranking function.

Experiment with different values for weights $\alpha_1$, $\alpha_2$, and $\alpha_3$ and try to find an improvement in average $F_1$ over the results achieved with each individual ranking function used in isolation.


\section{}

The goal now is to try a more sophisticated approach for combining the ranking functions used in the previous exercise. To this effect we will use a \emph{pointwise Learning to Rank} (L2R) approach.

Our approach consists in training a Logistic Regression classifier\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}} on the set of queries available in \texttt{pri\_queries.txt}. More specifically, you should:
\begin{enumerate}
\item Create a dataset for training and testing your L2R approach:
    \begin{itemize}
    \item Use 50\% of the queries for training and 50\% for testing (you can vary these percentages if you wish);
    \item With the training queries, build the \emph{training dataset}. This dataset should contain, for each \textit{(query $q$, document $d$)} pair, a set of classification instances with the format:
        \begin{displaymath}
            \operatorname{bm25}(q,d), \operatorname{cos}(q,d), \operatorname{freq}(q,d), r
        \end{displaymath}
        where $r = 1$ if document $d$ is relevant for query $q$ and $r=0$ otherwise. You can store this data on a \textit{numpy} array;
    \item Use the same number of relevant and non-relevant documents for each query.
    \end{itemize}
\item Use the training dataset to learn a Logistic Regression classifier:
    \begin{itemize}
    \item The three ranking scores will be your classification features and $r$ will be the target class.
    \end{itemize}
\item Execute the queries on the testing set, using the Logistic Regression classifier as your ranking function, and measure Precision, Recall, and $F_1$:
    \begin{itemize}
    \item To do this, first perform regular searches, using a each ranking function in isolation;
    \item The score of each ranking function will be the classification features and the classifier will return 1 if the document is relevant or 0 if otherwise;
    \item To order the resulting documents, you should use the \emph{probability of the document being relevant}. This can be obtained through the \texttt{predict\_proba} method of the \texttt{LogisticRegression} class.
    \end{itemize}
\end{enumerate}

\section{Pen and Paper Exercise}

Consider the problem of ranking search results with a learning-based method, leveraging the perceptron ranking algorithm. Consider also a training dataset in which there are two user queries, each with three candidate documents that should be presented to the user. Each document-query pair is represented as a feature vector $x$, together with a relevance judgement $y$ in a 3-point scale (i.e., $y \in {0,1,2,3}$):

\begin{itemize}
\item Query 1 and document 1 : x=<0.50,0.00,0.25,0.75> , y=1
\item Query 1 and document 2 : x=<0.25,0.00,0.00,0.25> , y=0
\item Query 1 and document 4 : x=<0.75,0.25,0.25,1.00> , y=3

\item Query 2 and document 1 : x=<0.50,0.00,0.25,1.00> , y=2
\item Query 2 and document 3 : x=<0.25,0.00,0.00,0.50> , y=0
\item Query 2 and document 4 : x=<0.25,0.00,0.25,0.50> , y=1
\end{itemize}

\begin{enumerate}
\item Simulate the execution of the training procedure for the perceptron ranking algorithm, considering one epoch over the training data. Consider an initial all-zeroes weight vector, and consider also an initial value of zero for each of the 3 thresholds associated to the possible values for the relevance estimates.

\item Consider a new user query, for which there are two candidate documents. Each of the document-query pairs is represented by a feature vector $x$ as follows:

\begin{itemize}
\item Query 3 and document 1 : x=<0.50,0.00,0.50,0.75>
\item Query 3 and document 1 : x=<0.50,0.25,0.50,0.75>
\end{itemize}

Using the trained perceptron from the previous exercise, estimate which of the documents should be ranked higher.
\end{enumerate}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

