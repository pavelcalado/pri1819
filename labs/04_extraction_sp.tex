\documentclass[12pt]{article}

\usepackage{prilab}
\usepackage{url}
\usepackage{amsmath}

\begin{document}

\maketitle{Lab 04: Information Extraction (cont.)}

The Python extension package named \verb+nltk+\footnote{\url{http://www.nltk.org}} provides a set of tools that are useful for addressing information extraction problems such as Named Entity Recognition (NER). More specifically, you can use the following methods:

\begin{itemize}
\item \verb+nltk.sent_tokenize(d)+, which splits a document d into a list of sentences;

\item \verb+nltk.word_tokenize(s)+, which splits a sentence s into a list of words;

\item \verb+nltk.pos_tag(w)+, which leverages a sequence classification model to tag the words in list $w$ according to their part-of-speech (i.e., tag words according to morphosyntactic classes such as noun, verb, adjective, $\ldots$);

\item \verb+nltk.ne_chunk(p, binary=True)+, which tags the words in list $p$ as named entities or not (where each word in $p$ was previously tagged with a part-of-speech tag).
\end{itemize}

Note that the output of each of these tools can be used as input to the next tool.

The \verb+nltk+\footnote{\url{https://www.nltk.org/api/nltk.tag.html}} documentation also presents several alternative models for parts-of-speech tagging and named entity recognition, leveraging different types of algorithms (e.g., structured perceptrons, CRFs, etc.)

\section{}

Test the Senna\footnote{\url{https://www.nltk.org/api/nltk.tag.html\#module-nltk.tag.senna}} POS tagger, NER tagger and chunk tagger with a few sentences of your own, or extracted from Web sites. Try text from different contexts (e.g. news, blogs, etc.).

\section{}

Using the above tools, print all named entities found in the documents of the 20 newsgroups collection\footnote{ \url{http://qwone.com/~jason/20Newsgroups/}}. This document collection can be conveniently accessed through the scikit-learn library, as shown in the previous lab class.

\section{Pen and Paper Exercise}

Consider the Hidden Markov Model from the previous exercises, represented by the following probabilities. Remember that $\pi$ corresponds to the initial probabilities of each state, $B$ corresponds to the state emission probabilities, and $A$ corresponds to the transition probabilities. 

The symbols corresponding to each line in matrix B are $a$, $b$, and $c$.

\begin{displaymath}
    \pi = 
    \begin{pmatrix}
        0.8 & 0.2
    \end{pmatrix}
%    ~\\~\\~\\
    B = 
    \begin{pmatrix}
        0.1 & 0.6 \\
        0.7 & 0.2 \\
        0.2 & 0.2 
    \end{pmatrix}
%    ~\\~\\~\\
    A = 
    \begin{pmatrix}
        0.1 & 0.5 \\
        0.9 & 0.5
    \end{pmatrix}
\end{displaymath}

Consider now a structured perceptron in which the considered feature representations/scores correspond to restructuring the HMM probabilities as scores.

\begin{enumerate}
\item Consider the observation \textbf{acb} associated to the sequence of states {\bf 121}. Show how this observation would be represented in terms of binary features.

\item Consider a structured perceptron defined with feature weights corresponding to the logarithm of the HMM probabilities. What is the most likely sequence of states for the sequence of symbols {\bf acbc}?

\item Starting from the structured perceptron model from the previous question, compute a new model using one iteration of the structured perceptron training method, assuming that you had only one observation available: \textbf{acb} associated to the sequence of states {\bf 122}.
\end{enumerate}

\end{document}
