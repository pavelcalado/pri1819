\documentclass[12pt]{article}

\usepackage{prilab}
\usepackage{url}
\usepackage{amsmath}

\begin{document}

\maketitle{Lab 03: Text Classification and Clustering}

In the following exercises, we will use the \emph{20 Newsgroup} dataset. This is a well known document collection, frequently used to evaluate text classifiers. You can find more
information at \url{http://qwone.com/~jason/20Newsgroups/}.

To solve the proposed problems, we will use the \emph{scikit-learn} machine learning library for Python\footnote{\url{http://scikit-learn.org/stable/}. All present code excerpts are adapted from the documentation.}. The goal is to experiment with simple approaches for text classification and clustering, reusing the implementations from \emph{scikit-learn}.

Whereas in previous lab classes you implemented mechanisms for reading textual documents and representing them according to the vector space model, in this case you will re-use existing methods for performing these operations. However, keep in mind that you should also be able to implement by yourself all the involved algorithms.

\subsection*{Background}

The scikit-learn library already provides access to the 20 Newsgroups dataset. You can use the following code to obtain it:

\begin{verbatim}
from sklearn.datasets import fetch_20newsgroups
train = fetch_20newsgroups(subset='train')
test = fetch_20newsgroups(subset='test')
\end{verbatim}

This already provides a standard split into training and test sets.

You can see the first 10 documents in the dataset through
\verb+train.data[:10]+ and the classes of those documents through \verb+train.target[:10]+. You will notice that the classes are represented as numbers. To see the class names you can use: \verb+train.target_names+.

The actual data is in text format. You need to transform it into numeric weight vectors (i.e., according to the vector space model, and using binary, term frequency, or TF-IDF weights). The scikit-learn library provides methods for this:
\begin{verbatim}
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer( use_idf=False )
trainvec = vectorizer.fit_transform(train.data)
testvec = vectorizer.transform(test.data)
\end{verbatim}

Once you do this to all data, you can fit a classifier on the training data and test it on the testing data.

\begin{verbatim}
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(trainvec, train.target)
classes = classifier.predict(testvec)
\end{verbatim}

The scikit-learn library also provides classes to evaluate classification results. The standard evaluation metrics, used in information retrieval and in text classification settings, will be detailed later in the course. The simplest of these metrics is perhaps the classification accuracy, which corresponds to the percentage of test instances that were assigned to the correct class:

\begin{verbatim}
from sklearn import metrics
print metrics.accuracy_score(test.target, classes)
print metrics.classification_report(test.target, classes)
\end{verbatim}

Document clustering can be achieved similarly with the scikit-learn library, for instance by using the K-Means algorithm:

\begin{verbatim}
from sklearn.cluster import MiniBatchKMeans
cluster = MiniBatchKMeans(20)
cluster.fit(trainvec)
\end{verbatim}

To check the clusters in which each document was placed, you can use \verb+cluster.labels_+.

\section{}

Implement a classifier for the 20 Newsgroups collection and measure its performance. Use the implementation of Multinomial Na{\"i}ve Bayes classifiers, available from scikit-learn.

\section{}

Try to improve the classification by:
\begin{enumerate}
\item Removing stopwords;
% \item Removing the message headers;
\item Removing very rare words (e.g. words that occur less than 3 times)
\item Removing very frequent words (e.g. words that occur in more than 90\% of the documents)\footnote{Note that these text preprocessing operations can be done using the \emph{Vectorizer} classes provided by scikit-learn.}
\item Use different classification algorithms, such as:
\begin{itemize}
\item A nearest neighbour classifier (i.e., \verb+sklearn.neighbors.KNeighborsClassifier+)
\item The Perceptron algorithm (i.e., \verb+sklearn.linear_model.Perceptron+)
\item Support Vector Machines (i.e., \verb+sklearn.svm.LinearSVC+)
\end{itemize}
\end{enumerate}

Measure the performance of each of these variations.

\section{}

Implement a clustering of the full 20 Newsgroup collection, using the implementation of K-Means clustering available from scikit-learn.

% \section{Implementation Exercise}

% Implement, in Python, (i) a nearest neighbour classifier, and (ii) a multinomial  na\"ive  Bayes  classifier. You can re-use the mechanisms in scikit-learn to represent documents as feature vectors, but you should program the training  procedures for inferring the model parameters yourself, as also the procedures for inferring the most likely class, given a test instance.

\section{Pen and Paper Exercise}

% \subsection{}

% Consider the following table where each record describes the characteristics of an individual, indicating also if the individual is a subscriber of a given news feed. The considered features are: gender, age ($<$ 26 years old) and if the individual owns a car.

% \begin{center}
% \scriptsize
% \tt
% \begin{tabular}{ l c c c }
% gender\_is\_male & ~~~~age < 26~~~~ & ~~~~has car~~~~ & ~~~~{\bf subscriber}~~~ \\
% \hline
% \hline
% Yes    & Yes                & No                & Yes \\ \hline
% Yes    & Yes                & Yes               & Yes \\ \hline
% Yes    & Yes                & No                & Yes \\ \hline
% Yes    & No                 & No                & No  \\ \hline
% Yes    & Yes                & Yes               & No  \\ \hline
% No     & No                 & No                & No  \\ \hline
% No     & Yes                & No                & No  \\ \hline
% No     & No                 & Yes               & No  \\ \hline
% No     & Yes                & Yes               & No  \\ \hline
% No     & Yes                & No                & No  \\ \hline
% \end{tabular}
% \end{center}

% Use the provided data to infer the parameters of a multinomial naïve Bayes classifier, capable of predicting if a given individual is a subscriber. After inferring the parameters, use them to classify the instance {\tt <Yes, No, Yes>}.

% Remember that a multinomial naïve Bayes classifier chooses a class $c$ according to the following rule, where $\mathbf{x}=<x_1,\ldots,x_n>$ is the instance (i.e., the feature vector) to classify:

% \begin{equation*}
% \underset{c}{\operatorname{argmax}} \text{~} \mathrm{P}(c) \times  \displaystyle\prod_{i=1}^n \mathrm{P}(x_i \vert c)
% \end{equation*}

% {\bf Note:} You can choose to only present the values for the parameters that are required to classify the given instance. It is not necessary to smooth the probability estimates. 

\subsection{}

Consider the following six textual documents, each associated to one of three possible classes.

\begin{table}[h!]
\centering
\begin{tabular}{ | c | l | c |}
  \hline			
ID & Document                          & Class \\
  \hline
D1 & the movie is nothing but great   & Positive \\
D2 & mixed feelings about the movie	  & Neutral \\
D3 & not so great			          & Negative \\
D4 & great fantastic movie			  & Positive \\
D5 & good movie overall			      & Positive \\
D6 & overall the movie is terrible    & Negative \\
  \hline  
\end{tabular}
\end{table}

\begin{itemize}
\item Estimate the parameters of a binary na{\"i}ve Bayes model required for classifying the document
    \begin{center}
        \emph{great movie overall}
    \end{center}
State which would be the most likely class for the given document, presenting all involved calculations.

    Use maximum likelihood estimation without considering any smoothing technique.

\item Estimate the parameters of a Perceptron classifier based on the first 3 training instances, discriminating the positive instances from all other instances (i.e., the negative and the neutral). Start with an all-zero parameter vector, consider binary representations for the documents, and consider a single iteration over the training instances.
\end{itemize}


\subsection{}

Consider the following collection of 4 text documents.

\begin{center}
\tt
\begin{tabular}{ l r }
number & text document \\
\hline
\hline
1 & shipment of gold damaged in fire \\ \hline
2 & delivery of silver arrived in silver truck \\ \hline
3 & shipment of silver arrived in truck \\ \hline
4 & truck damaged in fire \\ \hline
\end{tabular}
\end{center}

Based on the \emph{binary} document representations for the documents above, state which would be the result of applying {\it one iteration of the $k$-means algorithm} over the documents. 

When simulating the execution of one iteration of the algorithm, consider $k=2$, and consider that the two clusters are initialized with centroids equaling the representations for documents 2 and 3.

Two document representations 
 $\mathbf{a}$ and $\mathbf{b}$ can be compared through the Manhattan distance, given by:
\begin{equation*}
\mathrm{dist}(\mathbf{a}, \mathbf{b}) = \|\mathbf{a} - \mathbf{b}\|_1 = \sum_{i=1}^n |a_i-b_i|
\end{equation*}

\end{document}
